{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_url = 'https://api.pushshift.io/reddit/'\n",
    "\n",
    "\n",
    "# def request_posts(subreddit, days_ago, base_url=base_url,\n",
    "#                   endpoint='search/submission/', is_video='is_video=false'):\n",
    "#     try:\n",
    "#         response = requests.get(\n",
    "#             f'{base_url}{endpoint}?subreddit={subreddit}&{is_video}&before={days_ago}d&after={days_ago+1}d&size=1000')\n",
    "#         assert response.status_code == 200\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "#     return response\n",
    "\n",
    "\n",
    "# def make_requests(subreddit, days_of_data):\n",
    "#     all_results = []\n",
    "\n",
    "#     for i in range(1, days_of_data):\n",
    "#         try:\n",
    "#             entry = request_posts(subreddit, i)\n",
    "#             all_results.append(pd.DataFrame(entry.json()['data']))\n",
    "#         except:\n",
    "#             pass\n",
    "#         if i % 20 == 0:\n",
    "#             print(f'{i} of {days_of_data} requests completed')\n",
    "#         time.sleep(1.5)\n",
    "\n",
    "#     return pd.concat(all_results)\n",
    "\n",
    "\n",
    "# def request_all_subs(list_of_subreddits, days_of_data):\n",
    "#     all_results = []\n",
    "#     for sub in list_of_subreddits:\n",
    "#         print(f'Querying {sub}...')\n",
    "#         sub_df = make_requests(sub, days_of_data)\n",
    "#         all_results.append(sub_df)\n",
    "#     return pd.concat(all_results)\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     subs = ['asoiaf', 'freefolk', 'gameofthrones']\n",
    "#     df = request_all_subs(subs, 100)\n",
    "#     df.to_csv('./data/subreddit_data.csv')\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating API results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This script accepts the raw API response .csv and outputs one with NAs and deleted posts removed,\n",
    "# # plus any HTML fragments removed \n",
    "\n",
    "# # Imports\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "# import warnings\n",
    "\n",
    "# # Ignore warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# # Import Data\n",
    "# df = pd.read_csv('../data/subreddit_data.csv')\n",
    "\n",
    "# print('Number of Entries per subreddit pre-cleaning:')\n",
    "# print(df['subreddit'].value_counts())\n",
    "\n",
    "# # Drop posts without selftext\n",
    "# has_self = (df[~df['selftext'].isna()])\n",
    "# has_self = has_self[has_self['domain'].str.contains('self')]\n",
    "\n",
    "# # Drop posts where text has been removed or deleted by users/moderators\n",
    "# has_self = has_self[has_self['selftext']!='[removed]']\n",
    "# has_self = has_self[has_self['selftext']!='[deleted]']\n",
    "# has_self = has_self[~has_self['selftext'].str.startswith('[removed]')]\n",
    "# has_self = has_self[~has_self['selftext'].str.startswith('[deleted]')]\n",
    "\n",
    "# # Select the columns we're interested in modeling\n",
    "# columns = ['author', 'full_link', 'id', 'num_comments', 'num_crossposts', 'over_18',\n",
    "#                      'spoiler', 'subreddit', 'title', 'total_awards_received', 'upvote_ratio',\n",
    "#                      'edited', 'gilded','selftext']\n",
    "\n",
    "# has_self = has_self[columns]\n",
    "\n",
    "# # Reset index of results because it is annoying \n",
    "# has_self.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# print('Number of Entries per subreddit post-cleaning:')\n",
    "# print(has_self['subreddit'].value_counts())\n",
    "\n",
    "# # Just in case there are any HTML impurities in the raw data, we will use BeautifulSoup \n",
    "# # to clean the text columns\n",
    "# for col in has_self:\n",
    "#     if has_self[col].dtypes == 'O':\n",
    "#         for item in has_self.index:\n",
    "#             has_self.loc[item,col] = BeautifulSoup(has_self.loc[item,col]).get_text()\n",
    "\n",
    "# has_self.to_csv('../data/cleaned_subreddit_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating our full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import uniform, loguniform\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "#import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/cleaned_subreddit_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71555, 7)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70985, 7)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(df[df['author'] == 'AutoModerator'].index)\n",
    "\n",
    "#  df.loc[:,'selftext'] = df['selftext'].str.replace('x200B','')\n",
    "\n",
    "#  df.loc[:,'selftext'] = df['selftext'].str.replace('\\n',' ')\n",
    "\n",
    "# df = df[~df['selftext'].isna()]\n",
    "\n",
    "# df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['level_0', 'index', 'id', 'selftext', 'author', 'title', 'subreddit'], dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70985, 7)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_common_authors = df.author.value_counts().sort_values(ascending=False).head(21).index[1:]\n",
    "\n",
    "# table = df.loc[df['author'].isin(most_common_authors),['author','subreddit','id']].groupby(['author','subreddit']).count()\n",
    "\n",
    "# table.index.levels\n",
    "\n",
    "# ax = table.unstack().plot.barh(figsize=(10,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfid_reddit = TfidfVectorizer(lowercase=True, stop_words='english',\n",
    "#                               ngram_range=(1,2),max_df=.95, min_df=3, max_features=5000)\n",
    "\n",
    "# vect_table = tfid_reddit.fit_transform(df['selftext'])\n",
    "\n",
    "# vect_df = pd.DataFrame(vect_table.toarray(), columns = tfid_reddit.get_feature_names())\n",
    "\n",
    "# # vect_df_sorted = vect_df.sum().sort_values()\n",
    "\n",
    "# # #vect_df_sorted.head(100)\n",
    "\n",
    "# # vect_df_sorted.tail(100)\n",
    "\n",
    "# vect_df['subreddit'] = df['subreddit']\n",
    "# #vect_df['selftext'] = df['selftext']\n",
    "\n",
    "# # vect_df[vect_df['subreddit'] == 'freefolk'].drop(columns='subreddit').sum().sort_values(ascending=False).head(100)\n",
    "\n",
    "# # vect_df[vect_df['subreddit'] == 'gameofthrones'].drop(columns='subreddit').sum().sort_values(ascending=False).head(100)\n",
    "\n",
    "# # vect_df[vect_df['subreddit'] == 'asoiaf'].drop(columns='subreddit').sum().sort_values(ascending=False).head(100)\n",
    "\n",
    "# vect_df.isna().sum().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit    0\n",
       "title        0\n",
       "author       0\n",
       "selftext     0\n",
       "id           0\n",
       "index        0\n",
       "level_0      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Like yes I know you get Theons internal monologues and such is the book but I feel hes a lot more sympathetic and emotional in the show Alfie Allen does a fantastic job and you really see the conflict in his acting and his regret for what hes done Maester Luwin too i feel is a lot better and Rodriks death is definitely more impactful in the show than the book'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[45,'selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['selftext'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing workflow:\n",
    "\n",
    "- tokenize\n",
    "- remove stopwords\n",
    "- lemmatize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['selftext'] = [word_tokenize(str(text)) for text in df['selftext']]\n",
    "\n",
    "df['title'] = [word_tokenize(str(text)) for text in df['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'church'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('churches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['selftext'] = [[lemmatizer.lemmatize(word) for word in text] for text in df['selftext']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['selftext'] = [' '.join([stemmer.stem(word) for word in text]) for text in df['selftext']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for me it is jon snow He use to be my favorit but the overpand to hi charact by say he is the subject to everi propheci and is basic jesu is so annoy He is still a good charact especi in the book but think jon snow will defeat the other then he will kill dani and then rule wise and justli becaus he is the perfect hero is annoy'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.selftext[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Modeling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.sample(frac=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "asoiaf           1579\n",
       "freefolk          990\n",
       "gameofthrones     980\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.subreddit.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "asoiaf           0.444914\n",
       "freefolk         0.278952\n",
       "gameofthrones    0.276134\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.subreddit.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3549, 7)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sample['selftext']\n",
    "y = sample['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfid_final_params = {'tfid__max_df': 0.325,\n",
    "#   'tfid__max_features': 6500,\n",
    "#   'tfid__min_df': 5,\n",
    "#   'tfid__ngram_range': (1, 2)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline independent models: LogisticRegression and DecisionTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_pipe = Pipeline([\n",
    "    ('tfid',TfidfVectorizer(max_df=.325,\n",
    "                           max_features = 6500,\n",
    "                           min_df = 5,\n",
    "                           ngram_range = (1,2))),\n",
    "    ('logit',LogisticRegression(solver='liblinear'))\n",
    "])\n",
    "\n",
    "logit_params = {\n",
    "    'logit__C':loguniform(.01,50),\n",
    "    'logit__penalty':['l2','l1',None],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model = RandomizedSearchCV(logit_pipe,\n",
    "                  logit_params,\n",
    "                  n_iter = 75,\n",
    "                  n_jobs=-1,\n",
    "                  cv = 3,\n",
    "                  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 75 candidates, totalling 225 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   21.7s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 225 out of 225 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('tfid',\n",
       "                                              TfidfVectorizer(analyzer='word',\n",
       "                                                              binary=False,\n",
       "                                                              decode_error='strict',\n",
       "                                                              dtype=<class 'numpy.float64'>,\n",
       "                                                              encoding='utf-8',\n",
       "                                                              input='content',\n",
       "                                                              lowercase=True,\n",
       "                                                              max_df=0.325,\n",
       "                                                              max_features=6500,\n",
       "                                                              min_df=5,\n",
       "                                                              ngram_range=(1,\n",
       "                                                                           2),\n",
       "                                                              norm='l2',\n",
       "                                                              preprocessor=None,\n",
       "                                                              smooth_idf=True,\n",
       "                                                              stop_words=None,\n",
       "                                                              st...\n",
       "                                                                 solver='liblinear',\n",
       "                                                                 tol=0.0001,\n",
       "                                                                 verbose=0,\n",
       "                                                                 warm_start=False))],\n",
       "                                      verbose=False),\n",
       "                   iid='deprecated', n_iter=75, n_jobs=-1,\n",
       "                   param_distributions={'logit__C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000002059E657188>,\n",
       "                                        'logit__penalty': ['l2', 'l1', None]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logit__C': 2.552049865944663, 'logit__penalty': 'l2'}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.56554307, 0.58988764, 0.57677903, 0.57035647, 0.57973734])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(logit_model.best_estimator_,X_train,y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5505618 , 0.61797753, 0.61235955, 0.58988764, 0.53370787])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(logit_model.best_estimator_,X_test,y_test, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2984409480652932"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_model.score(X_test,y_test) - logit_model.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2984409480652932"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_model.best_estimator_.score(X_test,y_test) - logit_model.best_estimator_.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9186656671664168"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_model.best_estimator_.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6202247191011236"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_model.best_estimator_.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_pipe = Pipeline([\n",
    "    ('tfid',TfidfVectorizer(max_df=.325,\n",
    "                           max_features = 6500,\n",
    "                           min_df = 5,\n",
    "                           ngram_range = (1,2))),\n",
    "    ('tree',DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "tree_params = {\n",
    "    'tree__max_depth':uniform(1,20),\n",
    "    'tree__min_samples_split':uniform(0,.5),\n",
    "    'tree__min_samples_leaf':uniform(0,.5),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = RandomizedSearchCV(tree_pipe,\n",
    "                  tree_params,\n",
    "                  n_iter = 100,\n",
    "                  n_jobs=-1,\n",
    "                  cv = 3,\n",
    "                  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   20.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  2.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('tfid',\n",
       "                                              TfidfVectorizer(analyzer='word',\n",
       "                                                              binary=False,\n",
       "                                                              decode_error='strict',\n",
       "                                                              dtype=<class 'numpy.float64'>,\n",
       "                                                              encoding='utf-8',\n",
       "                                                              input='content',\n",
       "                                                              lowercase=True,\n",
       "                                                              max_df=0.325,\n",
       "                                                              max_features=6500,\n",
       "                                                              min_df=5,\n",
       "                                                              ngram_range=(1,\n",
       "                                                                           2),\n",
       "                                                              norm='l2',\n",
       "                                                              preprocessor=None,\n",
       "                                                              smooth_idf=True,\n",
       "                                                              stop_words=None,\n",
       "                                                              st...\n",
       "                   param_distributions={'tree__max_depth': <scipy.stats._distn_infrastructure.rv_frozen object at 0x00000205A0068948>,\n",
       "                                        'tree__min_samples_leaf': <scipy.stats._distn_infrastructure.rv_frozen object at 0x00000205A1315E08>,\n",
       "                                        'tree__min_samples_split': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000020594E673C8>},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tree__max_depth': 18.788855431067592,\n",
       " 'tree__min_samples_leaf': 0.0064049947690571485,\n",
       " 'tree__min_samples_split': 0.33934451621277056}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.47565543, 0.52621723, 0.52059925, 0.50281426, 0.48405253])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(tree_model.best_estimator_,X_train,y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44382022, 0.47752809, 0.52808989, 0.43820225, 0.41573034])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(tree_model.best_estimator_,X_test,y_test, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.057350538214039015"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_model.score(X_test,y_test) - tree_model.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.057350538214039015"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_model.best_estimator_.score(X_test,y_test) - tree_model.best_estimator_.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5629685157421289"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_model.best_estimator_.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5056179775280899"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_model.best_estimator_.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next step: implement voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Voting Classifier:\n",
    "# LogisticRegression\n",
    "# MultinomialNB\n",
    "# GradientBoostingClassifier\n",
    "# AdaBoostClassifier\n",
    "# RandomForestClassifier\n",
    "# DecisionTreeClassifier\n",
    "# SVC\n",
    "\n",
    "\n",
    "# Pipeline (tfid => Voting Classifier)\n",
    "\n",
    "# Random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.869304461753987"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
